MPI + OpenMP DaÄŸÄ±tÄ±k Hesaplama Sistemi

Bu proje, Docker Compose kullanarak daÄŸÄ±tÄ±k bir hesaplama ortamÄ± oluÅŸturur. MPI (Message Passing Interface) kullanarak dÃ¼ÄŸÃ¼mler arasÄ±nda veri paylaÅŸÄ±mÄ± yapÄ±lÄ±r ve OpenMP (Open Multi-Processing) ile her dÃ¼ÄŸÃ¼m kendi iÃ§inde Ã§ok Ã§ekirdekli paralel hesaplama yapar.

ğŸš€ Proje YapÄ±sÄ±

/MPI-OpenMP-Cluster
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ mpi_openmp.cpp
â””â”€â”€ README.md

ğŸ¯ Proje Hedefleri

Docker ile daÄŸÄ±tÄ±k bir sistem oluÅŸturmak.

Docker Compose kullanarak birden fazla dÃ¼ÄŸÃ¼m Ã§alÄ±ÅŸtÄ±rmak.

MPI ile dÃ¼ÄŸÃ¼mler arasÄ±nda veri iletiÅŸimi saÄŸlamak.

OpenMP ile her dÃ¼ÄŸÃ¼mÃ¼n iÃ§inde Ã§ok Ã§ekirdekli paralel hesaplama yapmasÄ±nÄ± saÄŸlamak.

DaÄŸÄ±tÄ±k sistemde hesaplama sonuÃ§larÄ±nÄ± toplamaya yÃ¶nelik bir pipeline kurmak.

ğŸ“¦ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

1ï¸âƒ£ Docker ve Docker Compose'u Kurun

EÄŸer sisteminizde Docker ve Docker Compose yoksa, aÅŸaÄŸÄ±daki komutlarla kurabilirsiniz:

Ubuntu / Debian

tsudo apt update
sudo apt install -y docker.io docker-compose

MacOS

brew install docker docker-compose

Windows (WSL2 Kullanman Ã–nerilir)

Docker Desktop 'u indir ve kur.

2ï¸âƒ£ Projeyi Ã‡alÄ±ÅŸtÄ±rma

AÅŸaÄŸÄ±daki komutlarla proje baÅŸlatÄ±labilir:

docker-compose build
docker-compose up

Not: EÄŸer eski konteynerlerden kaynaklÄ± sorun yaÅŸarsan, aÅŸaÄŸÄ±daki komutu kullanarak eski konteynerleri temizle:

docker-compose down --remove-orphans
docker system prune -a

âš™ï¸ Sistem Mimarisi

Bu proje, Master-Worker mimarisini kullanarak veri iÅŸleme yapar:

Master dÃ¼ÄŸÃ¼m (rank 0) ilk veriyi baÅŸlatÄ±r.

Her dÃ¼ÄŸÃ¼m MPI ile veriyi alÄ±r, OpenMP ile paralel iÅŸleyerek gÃ¼nceller ve sonraki dÃ¼ÄŸÃ¼me yollar.

Son dÃ¼ÄŸÃ¼m (rank N-1), nihai sonucu Master'a geri yollar.

ğŸ“œ Beklenen Ã‡Ä±ktÄ±

ğŸ¯ [Master] BaÅŸlangÄ±Ã§ deÄŸeri: 0
ğŸ“¤ [Master] DÃ¼ÄŸÃ¼m 1'e veri gÃ¶nderildi: 10

ğŸ“¥ [DÃ¼ÄŸÃ¼m 1] Veri alÄ±ndÄ±: 10
ğŸ”„ [DÃ¼ÄŸÃ¼m 1] Thread 0/4 iÅŸleniyor...
ğŸ“¤ [DÃ¼ÄŸÃ¼m 1] GÃ¼ncellenmiÅŸ veri: 15
â¡ï¸ [DÃ¼ÄŸÃ¼m 1] DÃ¼ÄŸÃ¼m 2'ye veri gÃ¶nderildi: 15

ğŸ“¥ [DÃ¼ÄŸÃ¼m 2] Veri alÄ±ndÄ±: 15
ğŸ”„ [DÃ¼ÄŸÃ¼m 2] Thread 0/4 iÅŸleniyor...
ğŸ“¤ [DÃ¼ÄŸÃ¼m 2] GÃ¼ncellenmiÅŸ veri: 25
â¡ï¸ [DÃ¼ÄŸÃ¼m 2] DÃ¼ÄŸÃ¼m 3'e veri gÃ¶nderildi: 25

ğŸ“¥ [DÃ¼ÄŸÃ¼m 3] Veri alÄ±ndÄ±: 25
ğŸ”„ [DÃ¼ÄŸÃ¼m 3] Thread 0/4 iÅŸleniyor...
ğŸ“¤ [DÃ¼ÄŸÃ¼m 3] GÃ¼ncellenmiÅŸ veri: 40
âœ… [DÃ¼ÄŸÃ¼m 3] Son dÃ¼ÄŸÃ¼m. Masterâ€™a sonuÃ§ gÃ¶nderildi: 40

ğŸ [Master] Nihai sonuÃ§ alÄ±ndÄ±: 40

ğŸ” Teknik Detaylar

MPI (Message Passing Interface): DaÄŸÄ±tÄ±k sistemlerde dÃ¼ÄŸÃ¼mler arasÄ± veri iletiÅŸimini saÄŸlar.

OpenMP (Open Multi-Processing): Ã‡ok Ã§ekirdekli paralel hesaplama yapÄ±lmasÄ±nÄ± saÄŸlar.

Docker Compose: AynÄ± anda birden fazla dÃ¼ÄŸÃ¼m baÅŸlatmak iÃ§in kullanÄ±lÄ±r.

Ubuntu TabanlÄ± Docker Konteynerleri: OpenMPI kullanÄ±larak Ã§ok dÃ¼ÄŸÃ¼mlÃ¼ bir yapÄ± oluÅŸturuldu.

ğŸ›  Sorun Giderme

ğŸ”¹ Docker aÄŸ hatasÄ± alÄ±yorum:

docker network prune
docker-compose down --remove-orphans
docker-compose up --build

ğŸ”¹ MPI dÃ¼ÄŸÃ¼mler haberleÅŸmiyor:

docker logs mpi_master
docker ps -a
docker-compose restart

ğŸ”¹ Eski konteynerleri temizle:

docker system prune -a
docker-compose up --force-recreate

ğŸ“Œ SonuÃ§

Bu proje, Docker Compose ile Ã§alÄ±ÅŸan Ã§ok dÃ¼ÄŸÃ¼mlÃ¼ bir daÄŸÄ±tÄ±k sistemdir. MPI ile dÃ¼ÄŸÃ¼mler haberleÅŸir, OpenMP ile her dÃ¼ÄŸÃ¼m iÃ§inde paralel hesaplama yapÄ±lÄ±r. Sistem paralel hesaplamalar gerektiren bÃ¼yÃ¼k veri iÅŸleme ve HPC (High-Performance Computing) senaryolarÄ± iÃ§in uygundur.

ğŸš€ Projeyi Ã§alÄ±ÅŸtÄ±rÄ±p test edebilirsin! EÄŸer Ã¶zelleÅŸtirme yapmak istersen bana bildirebilirsin! ğŸ˜



---



# ğŸ“Š MPI ve OpenMP Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±



Bu rapor, **MPI (Message Passing Interface)** ve **OpenMP (Open Multi-Processing)** teknolojilerinin **performans farklarÄ±nÄ± ve kullanÄ±m senaryolarÄ±nÄ±** karÅŸÄ±laÅŸtÄ±rarak hangi durumlarda daha verimli olduklarÄ±nÄ± aÃ§Ä±klamaktadÄ±r.



---



## ğŸ“Œ Genel KarÅŸÄ±laÅŸtÄ±rma



| **Ã–zellik**          | **MPI (Message Passing Interface)** | **OpenMP (Open Multi-Processing)** |

|----------------------|--------------------------------|--------------------------------|

| **KullanÄ±m AlanÄ±**   | DaÄŸÄ±tÄ±k sistemlerde dÃ¼ÄŸÃ¼mler arasÄ± iletiÅŸim | Tek sistemde Ã§ok Ã§ekirdekli iÅŸlem |

| **Ä°letiÅŸim Modeli**  | DÃ¼ÄŸÃ¼mler arasÄ± mesaj iletimi (process bazlÄ±) | PaylaÅŸÄ±lan bellek Ã¼zerinden doÄŸrudan eriÅŸim (thread bazlÄ±) |

| **Performans**       | BÃ¼yÃ¼k veri kÃ¼meleri ve kÃ¼me bilgisayarlarda avantajlÄ± | Tek makine Ã¼zerindeki hesaplamalarda hÄ±zlÄ± |

| **Ã–lÃ§eklenebilirlik**| BÃ¼yÃ¼k Ã¶lÃ§ekli kÃ¼me sistemlerinde kolay Ã¶lÃ§eklenir | Makinedeki Ã§ekirdek sayÄ±sÄ±yla sÄ±nÄ±rlÄ±dÄ±r |

| **Gecikme**          | AÄŸ gecikmeleri nedeniyle iÅŸlem sÃ¼resi artabilir | Bellek paylaÅŸÄ±mÄ± nedeniyle dÃ¼ÅŸÃ¼k gecikme |

| **KarmaÅŸÄ±klÄ±k**      | Paralel programlama iÃ§in daha karmaÅŸÄ±ktÄ±r | Daha basit ve yÃ¶netilebilir |



---



## ğŸ“Œ Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±



Bu bÃ¶lÃ¼mde, **farklÄ± iÅŸ yÃ¼kleri altÄ±nda MPI ve OpenMP'nin nasÄ±l performans gÃ¶sterdiÄŸini** inceleyeceÄŸiz.



### ğŸ–¥ï¸ 1ï¸âƒ£ KÃ¼Ã§Ã¼k Ã–lÃ§ekli Paralel Ä°ÅŸlem (Tek Makine)

- **OpenMP, aynÄ± makinede Ã§ok Ã§ekirdekli iÅŸlem yapmada daha hÄ±zlÄ±dÄ±r** Ã§Ã¼nkÃ¼ bellek paylaÅŸÄ±mÄ±nÄ± doÄŸrudan kullanÄ±r.

- **MPI burada daha yavaÅŸ olabilir** Ã§Ã¼nkÃ¼ her iÅŸlem birbirinden baÄŸÄ±msÄ±zdÄ±r ve haberleÅŸme gereklidir.



**ğŸ”¹ Ã–rnek Senaryo:** KÃ¼Ã§Ã¼k boyutlu matris Ã§arpÄ±mÄ±  

âœ… **Kazanan: OpenMP**



---



### ğŸŒ 2ï¸âƒ£ BÃ¼yÃ¼k Veri Setleri ile DaÄŸÄ±tÄ±k Hesaplama

- **MPI, bÃ¼yÃ¼k veri kÃ¼meleriyle daha iyi Ã¶lÃ§eklenir** Ã§Ã¼nkÃ¼ birden fazla makineye yayÄ±labilir.

- **OpenMP, tek makinede sÄ±nÄ±rlÄ±dÄ±r** ve veri seti bÃ¼yÃ¼dÃ¼kÃ§e verimsiz hale gelir.



**ğŸ”¹ Ã–rnek Senaryo:** KÃ¼me bilgisayarlarÄ±nda bÃ¼yÃ¼k veri analizi  

âœ… **Kazanan: MPI**



---



### âš¡ 3ï¸âƒ£ Hibrit KullanÄ±m Senaryosu (MPI + OpenMP)

- **MPI, dÃ¼ÄŸÃ¼mler arasÄ± veri paylaÅŸÄ±mÄ±nÄ± yÃ¶netirken**, **OpenMP her dÃ¼ÄŸÃ¼m iÃ§inde paralel iÅŸlem yapabilir**.

- Bu yapÄ± **bÃ¼yÃ¼k Ã¶lÃ§ekli hesaplamalarda en yÃ¼ksek verimi saÄŸlar**.



**ğŸ”¹ Ã–rnek Senaryo:** BÃ¼yÃ¼k Ã¶lÃ§ekli bilimsel simÃ¼lasyonlar (Hava durumu modelleme, fizik hesaplamalarÄ±)  

âœ… **Kazanan: Hibrit Model (MPI + OpenMP)**



---



## ğŸ“Œ SonuÃ§



- **KÃ¼Ã§Ã¼k Ã¶lÃ§ekli, tek makine Ã¼zerindeki paralel hesaplamalar iÃ§in OpenMP daha hÄ±zlÄ±dÄ±r.**

- **BÃ¼yÃ¼k Ã¶lÃ§ekli, Ã§ok dÃ¼ÄŸÃ¼mlÃ¼ sistemlerde MPI daha verimlidir.**

- **En iyi performans, MPI ile daÄŸÄ±tÄ±k hesaplama + OpenMP ile yerel paralel iÅŸlem kombinasyonuyla elde edilir.**



ğŸš€ **Bu projede hem MPI hem de OpenMP kullanarak en iyi performans saÄŸlandÄ±.**

